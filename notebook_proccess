from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, explode
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, ArrayType, LongType
from IPython.display import display
import pandas as pd
import time

# Konfiguracja SparkSession
spark = SparkSession.builder \
    .appName("Real-Time NEO Data Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Definiowanie schematu danych JSON
schema = StructType([
    StructField("links", StructType([
        StructField("self", StringType(), True)
    ]), True),
    StructField("id", StringType(), True),
    StructField("neo_reference_id", StringType(), True),
    StructField("name", StringType(), True),
    StructField("nasa_jpl_url", StringType(), True),
    StructField("absolute_magnitude_h", DoubleType(), True),
    StructField("estimated_diameter", StructType([
        StructField("kilometers", StructType([
            StructField("estimated_diameter_min", DoubleType(), True),
            StructField("estimated_diameter_max", DoubleType(), True)
        ]), True),
        StructField("meters", StructType([
            StructField("estimated_diameter_min", DoubleType(), True),
            StructField("estimated_diameter_max", DoubleType(), True)
        ]), True),
        StructField("miles", StructType([
            StructField("estimated_diameter_min", DoubleType(), True),
            StructField("estimated_diameter_max", DoubleType(), True)
        ]), True),
        StructField("feet", StructType([
            StructField("estimated_diameter_min", DoubleType(), True),
            StructField("estimated_diameter_max", DoubleType(), True)
        ]), True)
    ]), True),
    StructField("is_potentially_hazardous_asteroid", BooleanType(), True),
    StructField("close_approach_data", ArrayType(StructType([
        StructField("close_approach_date", StringType(), True),
        StructField("close_approach_date_full", StringType(), True),
        StructField("epoch_date_close_approach", LongType(), True),
        StructField("relative_velocity", StructType([
            StructField("kilometers_per_second", StringType(), True),
            StructField("kilometers_per_hour", StringType(), True),
            StructField("miles_per_hour", StringType(), True)
        ]), True),
        StructField("miss_distance", StructType([
            StructField("astronomical", StringType(), True),
            StructField("lunar", StringType(), True),
            StructField("kilometers", StringType(), True),
            StructField("miles", StringType(), True)
        ]), True),
        StructField("orbiting_body", StringType(), True)
    ])), True),
    StructField("is_sentry_object", BooleanType(), True)
])

# Odbieranie danych z Kafka
raw_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "neo-topic") \
    .option("startingOffsets", "earliest") \
    .load()

# Przetwarzanie danych
parsed_df = raw_df \
    .selectExpr("CAST(value AS STRING) AS value") \
    .select(
        from_json(col("value"), schema).alias("data"),
        col("value")
    ) \
    .select(
        col("data.links.self").alias("link_self"),
        col("data.id"),
        col("data.neo_reference_id"),
        col("data.name"),
        col("data.nasa_jpl_url"),
        col("data.absolute_magnitude_h"),
        col("data.estimated_diameter.kilometers.estimated_diameter_min").alias("diameter_min_km"),
        col("data.estimated_diameter.kilometers.estimated_diameter_max").alias("diameter_max_km"),
        col("data.estimated_diameter.meters.estimated_diameter_min").alias("diameter_min_m"),
        col("data.estimated_diameter.meters.estimated_diameter_max").alias("diameter_max_m"),
        col("data.estimated_diameter.miles.estimated_diameter_min").alias("diameter_min_miles"),
        col("data.estimated_diameter.miles.estimated_diameter_max").alias("diameter_max_miles"),
        col("data.estimated_diameter.feet.estimated_diameter_min").alias("diameter_min_feet"),
        col("data.estimated_diameter.feet.estimated_diameter_max").alias("diameter_max_feet"),
        col("data.is_potentially_hazardous_asteroid"),
        explode(col("data.close_approach_data")).alias("close_approach_data"),
        col("data.is_sentry_object")
    ) \
    .select(
        "link_self", "id", "neo_reference_id", "name", "nasa_jpl_url", "absolute_magnitude_h",
        "diameter_min_km", "diameter_max_km", "diameter_min_m", "diameter_max_m",
        "diameter_min_miles", "diameter_max_miles", "diameter_min_feet", "diameter_max_feet",
        "is_potentially_hazardous_asteroid",
        col("close_approach_data.close_approach_date").alias("approach_date"),
        col("close_approach_data.close_approach_date_full").alias("approach_date_full"),
        col("close_approach_data.epoch_date_close_approach").alias("epoch_date"),
        col("close_approach_data.relative_velocity.kilometers_per_second").alias("velocity_kps"),
        col("close_approach_data.relative_velocity.kilometers_per_hour").alias("velocity_kph"),
        col("close_approach_data.relative_velocity.miles_per_hour").alias("velocity_mph"),
        col("close_approach_data.miss_distance.astronomical").alias("distance_astronomical"),
        col("close_approach_data.miss_distance.lunar").alias("distance_lunar"),
        col("close_approach_data.miss_distance.kilometers").alias("distance_km"),
        col("close_approach_data.miss_distance.miles").alias("distance_miles"),
        col("close_approach_data.orbiting_body").alias("orbiting_body"),
        "is_sentry_object"
    )

# Filtruj błędne rekordy
valid_records = parsed_df.filter(col("is_potentially_hazardous_asteroid").isNotNull())

# Zapisywanie wyników do Memory Sink
query = valid_records \
    .writeStream \
    .outputMode("append") \
    .format("memory") \
    .queryName("neo_results") \
    .start()

# Funkcja do kolorowania wierszy
def highlight_hazardous(row):
    color = "red" if row["is_potentially_hazardous_asteroid"] else ""
    return [f"background-color: {color}" for _ in row]

# Sprawdzaj wyniki co 10 sekund
try:
    while True:
        neo_df = spark.sql("SELECT * FROM neo_results")
        
        if not neo_df.rdd.isEmpty():
            pdf = neo_df.toPandas()
            styled_pdf = pdf.style.apply(highlight_hazardous, axis=1)
            print("\nAktualne dane:")
            display(styled_pdf)
        else:
            print("Brak nowych danych...")
        
        time.sleep(60)

except KeyboardInterrupt:
    print("Zatrzymywanie strumienia...")
finally:
    query.stop()
    spark.stop()