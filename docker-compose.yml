version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    networks:
      - spark-kafka-network
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    networks:
      - spark-kafka-network
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_MESSAGE_MAX_BYTES: 1048576000  # 1 GB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 1048576000  # 1 GB
      KAFKA_FETCH_MAX_BYTES: 1048576000 
    ports:
      - "9092:9092"

  spark:
    build: .  # Użyj Dockerfile do zbudowania obrazu
    networks:
      - spark-kafka-network
    ports:
      - "4040:4040"
    volumes:
      - ./spark-apps:/app
    environment:
      - SPARK_MODE=master
    command: >
      bash -c "
      sleep 10 &&  # Dodane opóźnienie dla Kafki
      /opt/bitnami/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4
      /app/spark_streaming.py
      "

  jupyter:
    image: jupyter/pyspark-notebook:latest
    networks:
      - spark-kafka-network
    ports:
      - "8888:8888"
      - "8889:8889"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - SPARK_MASTER=spark://spark:7077

  producer:  # POPRAWIONE WCIĘCIE - teraz to osobna usługa
    image: python:3.9
    networks:
      - spark-kafka-network
    volumes:
      - ./producer.py:/app/producer.py
    depends_on:
      - kafka
    command: >
      bash -c "
      sleep 15 &&  # Czekaj na Sparka i Kafkę
      python /app/producer.py
      "

networks:
  spark-kafka-network:
    driver: bridge